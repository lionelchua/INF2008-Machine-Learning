{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f009f8-865d-4975-aef4-a54f3e9b11cd",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9961cfdb-2291-460c-b676-f4f1df028fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a301990-f78f-4077-bf44-90ee90def070",
   "metadata": {},
   "source": [
    "# Data Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f640b1f0-ed11-4d74-beef-026ae8cf6701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the season\n",
    "season = \"2017-2018\"  \n",
    "\n",
    "# Construct the dynamic URL\n",
    "standings_url = f\"https://fbref.com/en/comps/9/{season}/{season}-Premier-League-Stats\"\n",
    "data = requests.get(standings_url)\n",
    "\n",
    "soup = BeautifulSoup(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d0ac439-e9a7-4122-8c79-57834645444e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://fbref.com/en/squads/b8fd03ef/2017-2018/Manchester-City-Stats',\n",
       " 'https://fbref.com/en/squads/19538871/2017-2018/Manchester-United-Stats',\n",
       " 'https://fbref.com/en/squads/361ca564/2017-2018/Tottenham-Hotspur-Stats',\n",
       " 'https://fbref.com/en/squads/822bd0ba/2017-2018/Liverpool-Stats',\n",
       " 'https://fbref.com/en/squads/cff3d9bb/2017-2018/Chelsea-Stats',\n",
       " 'https://fbref.com/en/squads/18bb7c10/2017-2018/Arsenal-Stats',\n",
       " 'https://fbref.com/en/squads/943e8050/2017-2018/Burnley-Stats',\n",
       " 'https://fbref.com/en/squads/d3fd31cc/2017-2018/Everton-Stats',\n",
       " 'https://fbref.com/en/squads/a2d435b3/2017-2018/Leicester-City-Stats',\n",
       " 'https://fbref.com/en/squads/b2b47a98/2017-2018/Newcastle-United-Stats',\n",
       " 'https://fbref.com/en/squads/47c64c55/2017-2018/Crystal-Palace-Stats',\n",
       " 'https://fbref.com/en/squads/4ba7cbea/2017-2018/Bournemouth-Stats',\n",
       " 'https://fbref.com/en/squads/7c21e445/2017-2018/West-Ham-United-Stats',\n",
       " 'https://fbref.com/en/squads/2abfe087/2017-2018/Watford-Stats',\n",
       " 'https://fbref.com/en/squads/d07537b9/2017-2018/Brighton-and-Hove-Albion-Stats',\n",
       " 'https://fbref.com/en/squads/f5922ca5/2017-2018/Huddersfield-Town-Stats',\n",
       " 'https://fbref.com/en/squads/33c895d4/2017-2018/Southampton-Stats',\n",
       " 'https://fbref.com/en/squads/fb10988f/2017-2018/Swansea-City-Stats',\n",
       " 'https://fbref.com/en/squads/17892952/2017-2018/Stoke-City-Stats',\n",
       " 'https://fbref.com/en/squads/60c6b05f/2017-2018/West-Bromwich-Albion-Stats']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find all <table> elements with class \"stats_table\"\n",
    "PL_table = soup.select('table.stats_table')[0]\n",
    "\n",
    "#Finds all <a> elements in the Premier League table\n",
    "links = PL_table.find_all('a')\n",
    "\n",
    "#Extract only the href attribute from each <a> tag and store all links as a list\n",
    "links = [l.get(\"href\") for l in links]\n",
    "\n",
    "#Keep only links containing /squads/\n",
    "links = [l for l in links if '/squads/' in l]\n",
    "\n",
    "#Reconstruct full URLs\n",
    "squad_urls = [f\"https://fbref.com{l}\" for l in links]\n",
    "\n",
    "squad_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c525b25d-dac7-4c69-8b8a-02e7ad7aeea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2025-01-31 11:38:45\n",
      "1. Manchester City complete\n",
      "2. Manchester United complete\n",
      "3. Tottenham Hotspur complete\n",
      "4. Liverpool complete\n",
      "5. Chelsea complete\n",
      "6. Arsenal complete\n",
      "7. Burnley complete\n",
      "8. Everton complete\n",
      "9. Leicester City complete\n",
      "10. Newcastle United complete\n",
      "11. Crystal Palace complete\n",
      "12. Bournemouth complete\n",
      "13. West Ham United complete\n",
      "14. Watford complete\n",
      "15. Brighton and Hove Albion complete\n",
      "16. Huddersfield Town complete\n",
      "17. Southampton complete\n",
      "18. Swansea City complete\n",
      "19. Stoke City complete\n",
      "20. West Bromwich Albion complete\n",
      "End time: 2025-01-31 12:01:14\n",
      "Execution Time: 0:22:28.991431\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(f\"Start time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "all_teams_data = []\n",
    "loop_count = 0\n",
    "\n",
    "for squad_url in squad_urls:\n",
    "    loop_count += 1\n",
    "    team_name = squad_url.split(\"/\")[-1].replace(\"-Stats\",\"\").replace(\"-\",\" \")\n",
    "\n",
    "    #Scores & Fixtures\n",
    "    data = requests.get(squad_url)\n",
    "    matches = pd.read_html(data.text, match =\"Scores & Fixtures\")\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    shooting_links = soup.find_all('a')\n",
    "    shooting_links = [l.get(\"href\") for l in shooting_links]\n",
    "    shooting_links = [l for l in shooting_links if l and 'all_comps/shooting/' in l]\n",
    "    shooting_page = requests.get(f\"https://fbref.com{shooting_links[0]}\")\n",
    "    shooting_stats = pd.read_html(shooting_page.text, match =\"Shooting\")[0]\n",
    "    shooting_stats.columns = shooting_stats.columns.droplevel()\n",
    "    team_data = matches[0].drop(columns=[\"Match Report\", \"Notes\"])\n",
    "    team_data = team_data.merge(shooting_stats[[\"Date\",\"Sh\",\"SoT\",\"SoT%\",\"G/Sh\",\"G/SoT\",\"Dist\",\"FK\",\"PK\",\"PKatt\",\"npxG\",\"npxG/Sh\",\"G-xG\",\"np:G-xG\"]], on=\"Date\")\n",
    "    time.sleep(7)\n",
    "\n",
    "    #Goalkeeping\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    goalkeeping_links = soup.find_all('a')\n",
    "    goalkeeping_links = [l.get(\"href\") for l in goalkeeping_links]\n",
    "    goalkeeping_links = [l for l in goalkeeping_links if l and 'all_comps/keeper/' in l]\n",
    "    goalkeeping_page = requests.get(f\"https://fbref.com{goalkeeping_links[0]}\")\n",
    "    goalkeeping_stats = pd.read_html(goalkeeping_page.text, match =\"Goalkeeping\")[0]\n",
    "    goalkeeping_stats.columns = goalkeeping_stats.columns.droplevel()\n",
    "    team_data = team_data.merge(goalkeeping_stats[[\"Date\",\"SoTA\",\"Saves\",\"Save%\",\"CS\"]], on=\"Date\")\n",
    "    time.sleep(7)\n",
    "\n",
    "    #Passing\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    passing_links = soup.find_all('a')\n",
    "    passing_links = [l.get(\"href\") for l in passing_links]\n",
    "    passing_links = [l for l in passing_links if l and 'all_comps/passing/' in l]\n",
    "    passing_page = requests.get(f\"https://fbref.com{passing_links[0]}\")\n",
    "    passing_stats = pd.read_html(passing_page.text, match =\"Passing\")[0]\n",
    "    passing_stats.columns = passing_stats.columns.droplevel()\n",
    "    passing_stats.columns.values[10:24] = [\"Total_Cmp\", \"Total_Att\", \"Total_Cmp%\", \"Total_TotDist\", \"Total_PrgDist\",\n",
    "                                           \"Short Pass Cmp\",\"Short Pass Att\",\"Short Pass Cmp%\",\"Medium Pass Cmp\",\"Medium Pass Att\",\"Medium Pass Cmp%\",\n",
    "                                           \"Long Pass Cmp\",\"Long Pass Att\",\"Long Pass Cmp%\"]\n",
    "    passing_merge_cols = passing_stats.iloc[:, np.r_[0, 10:32]]\n",
    "    team_data = team_data.merge(passing_merge_cols, on=\"Date\", how=\"left\")\n",
    "    time.sleep(7)\n",
    "\n",
    "    #Pass Types\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    pass_types_links = soup.find_all('a')\n",
    "    pass_types_links = [l.get(\"href\") for l in pass_types_links]\n",
    "    pass_types_links = [l for l in pass_types_links if l and 'all_comps/passing_types/' in l]\n",
    "    pass_types_page = requests.get(f\"https://fbref.com{pass_types_links[0]}\")\n",
    "    pass_types_stats = pd.read_html(pass_types_page.text, match =\"Pass Types\")[0]\n",
    "    pass_types_stats.columns = pass_types_stats.columns.droplevel()\n",
    "    pass_types_stats.columns.values[10:25] = [\"Passes Attempted\",\"Live-ball Passes\", \"Dead-ball Passes\", \"Passes from Free Kicks\", \"Through Balls\", \"Switches\",\n",
    "                                           \"Crosses\",\"Throw-ins Taken\",\"Corner Kicks\",\"Inswinging Corner Kick\",\"Outswinging Corner Kicks\",\"Straight Corner Kicks\",\n",
    "                                           \"Passes Completed\",\"Passes Offside\",\"Passes Blocked\"]   \n",
    "    pass_types_merge_cols = pass_types_stats.iloc[:, np.r_[0, 10:25]]\n",
    "    team_data = team_data.merge(pass_types_merge_cols, on=\"Date\", how=\"left\")\n",
    "    time.sleep(7)\n",
    "\n",
    "    #Goal and Shot Creation\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    gsc_links = soup.find_all('a')\n",
    "    gsc_links = [l.get(\"href\") for l in gsc_links]\n",
    "    gsc_links = [l for l in gsc_links if l and 'all_comps/gca/' in l]\n",
    "    gsc_page = requests.get(f\"https://fbref.com{gsc_links[0]}\")\n",
    "    gsc_stats = pd.read_html(gsc_page.text, match =\"Goal and Shot Creation\")[0]\n",
    "    gsc_stats.columns = gsc_stats.columns.droplevel()\n",
    "    gsc_stats.columns.values[11:24] = [\"SCA (Live-ball Pass)\",\"SCA (Dead-ball Pass)\", \"SCA (Take-On)\", \"SCA (Shot)\", \"SCA (Fouls Drawn)\", \"SCA (Defensive Action)\",\n",
    "                                           \"GCA\",\"GCA (Live-ball Pass)\",\"GCA (Dead-ball Pass)\",\"GCA (Take-On)\",\"GCA (Shot)\",\"GCA (Fouls Drawn)\",\n",
    "                                           \"GCA (Defensive Action)\"]\n",
    "    gsc_merge_cols = gsc_stats.iloc[:, np.r_[0, 10:24]]\n",
    "    team_data = team_data.merge(gsc_merge_cols, on=\"Date\", how=\"left\")\n",
    "    time.sleep(7)\n",
    "\n",
    "    #Defensive Actions\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    dfa_links = soup.find_all('a')\n",
    "    dfa_links = [l.get(\"href\") for l in dfa_links]\n",
    "    dfa_links = [l for l in dfa_links if l and 'all_comps/defense/' in l]\n",
    "    dfa_page = requests.get(f\"https://fbref.com{dfa_links[0]}\")\n",
    "    dfa_stats = pd.read_html(dfa_page.text, match =\"Defensive Actions\")[0]\n",
    "    dfa_stats.columns = dfa_stats.columns.droplevel()\n",
    "    dfa_stats.columns.values[10:26] = [\"Tackles\",\"Tackles Won\", \"Tackles (Def 3rd)\", \"Tackles (Mid 3rd)\", \"Tackles (Att 3rd)\", \"Dribblers Tackled\",\n",
    "                                           \"Dribbles Challenged\",\"% of Dribblers Tackled\",\"Challenges Lost\",\"Blocks \",\"Shots Blocked\",\"Passes Blocked\",\n",
    "                                           \"Interceptions\",\"Tkl+Int\",\"Clearances\",\"Errors\"]\n",
    "    dfa_merge_cols = dfa_stats.iloc[:, np.r_[0, 10:26]]\n",
    "    team_data = team_data.merge(dfa_merge_cols, on=\"Date\", how=\"left\")\n",
    "    time.sleep(7)\n",
    "\n",
    "    #Possession\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    possession_links = soup.find_all('a')\n",
    "    possession_links = [l.get(\"href\") for l in possession_links]\n",
    "    possession_links = [l for l in possession_links if l and 'all_comps/possession/' in l]\n",
    "    possession_page = requests.get(f\"https://fbref.com{possession_links[0]}\")\n",
    "    possession_stats = pd.read_html(possession_page.text, match =\"Possession\")[0]\n",
    "    possession_stats.columns = possession_stats.columns.droplevel()\n",
    "    \n",
    "    possession_stats.columns.values[10:33] = [\"Possession %\",\"Touches\", \"Touches (Def Pen)\", \"Touches (Def 3rd)\", \"Touches (Mid 3rd)\", \"Touches (Att 3rd)\",\n",
    "                                              \"Touches (Att Pen)\",\"Touches (Live-Ball)\",\"Take-Ons Attempted\",\"Successful Take-Ons \",\"Successful Take-On %\",\n",
    "                                              \"Times Tackled During Take-On\",\"Tackled During Take-On Percentage\",\"Carries \",\"Total Carrying Distance\",\n",
    "                                              \"Progressive Carrying Distance\",\"Progressive Carries\",\"Carries into Final Third\",\"Carries into Penalty Area\",\n",
    "                                              \"Miscontrols\",\"Dispossessed\",\"Passes Received\",\"Progressive Passes Rec\"]\n",
    "    possession_merge_cols = possession_stats.iloc[:, np.r_[0, 10:33]]\n",
    "    team_data = team_data.merge(possession_merge_cols, on=\"Date\", how=\"left\")\n",
    "    time.sleep(7)\n",
    "\n",
    "    #Only keep Premier League games\n",
    "    team_data = team_data[team_data[\"Comp\"] == \"Premier League\"]\n",
    "\n",
    "    #Add season and team colummn data\n",
    "    team_data[\"Season\"] = season\n",
    "    team_data[\"Team\"] = team_name\n",
    "\n",
    "    all_teams_data.append(team_data)\n",
    "    print(f\"{loop_count}. {team_name} complete\")\n",
    "\n",
    "all_teams_data_df = pd.concat(all_teams_data, ignore_index=True)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f\"End time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Calculate execution time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution Time: {execution_time}\")\n",
    "\n",
    "csv_filename = f\"{season}.csv\"\n",
    "all_teams_data_df.to_csv(csv_filename, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29e12d9-5e9d-4044-9236-f269c2072480",
   "metadata": {},
   "source": [
    "# Load Data Scrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "4369a4e4-a99c-4ad8-a576-5291e491d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017 = pd.read_csv(\"2017-2018.csv\")\n",
    "df_2018 = pd.read_csv(\"2018-2019.csv\")\n",
    "df_2019 = pd.read_csv(\"2019-2020.csv\")\n",
    "df_2020 = pd.read_csv(\"2020-2021.csv\")\n",
    "df_2021 = pd.read_csv(\"2021-2022.csv\")\n",
    "df_2022 = pd.read_csv(\"2022-2023.csv\")\n",
    "df_2023 = pd.read_csv(\"2023-2024.csv\")\n",
    "df_2024 = pd.read_csv(\"2024-2025.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "8a3edb95-1699-4314-864d-28a83d91553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([df_2017, df_2018, df_2019, df_2020, df_2021, df_2022, df_2023, df_2024], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "fb1b1563-6249-4e9f-ad87-3313b9b36ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(\"2017-2024_match_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
